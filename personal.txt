source /shared/katop1234/miniconda3/bin/activate /shared/katop1234/miniconda3/envs/FPT
python main.py

Zeta Finance

Project Overview
- Train a self supervised model on a large corpus of financial data through decoder-only architecture
- Feed in various historical features like last 32-64 days volume, last 64-128 days open price, etc. and then
encode them like in a ViT and just train the model to predict the price after one day (we can easily finetune from
this for any N days)
- Data would be normalized to the return wrt to previous day. So instead of price of FB being 90 99 90, it would be 
0 0.1 -0.1 or whatever for consistency
- sample every 15 minutes. the tokens will be last 1 day, 2, 4, 8, 16, 32, 64, 128, 256, 512 and then 512 going back to however many
data points we have.
- Train a giant decoder model on this data as regression for expected return after 1 day (after doing this, it should easily be fine-
tunable for any window like 1 week or 4 weeks if that's better. but this is a good start). Doing classification on 
up/stay/down may work also
- For actual running, can show the model of a random sample of current stock data and have it predict for each one
 whether to buy or sell. Or can finetune to specific commodities like USD or crypto option or whatever also.
 Eventually can come up with something smarter that knows how to sample stocks to find good ones.
- For now, we can ask it to buy/sell and close the position the next day. That’s all. dont do hft, just try to trade
 every 1 week. This way you don’f have to worry anout game theoretic effects of having slow equipment and not seeing
  trades before ohers like if you did every 1 minute or so.
- Profit!
- Eventually include arbritrage, so the deep learning model automatically detects when to arb different products even
 on different markets

To-Do for the MVP:
- normalize data to be returns. and preprocess the year column to subtract from 1900 and then divide by 100
- for data preprocessing, make it percent change from previous day's value for each value
- make the embedder versatile as possible with helper functions so that it works for any dataset with various input types
- figure out how to update the attention mask to feed into the transformer
- add environment.yml to load this anywhere
- add backprop code
- add code to store/load from checkpoints

Future Steps:
- Since we normalize all the data wrt to the previous day's value, we forget the original "scale". I.e. $100 or $1000 all become
percent changes. Figure out a way to include the original price. Maybe you can cross attend the first value of the price 
onto the learned embedding before feeding it into the decoder (?)
- implement MOE (mixture of experts https://arxiv.org/abs/2305.14705).
- implement model parallelism and implement data parallelism and loader for fpt
- write the code to do backprop based on fb mae
- more indicators to add:
put call ratio
volatility index
bullish percent index
advance decline line
high low index
investor sentiment survey
- Add options data also(?) maybe later, can add more modalities as we go on (add crypto, forex etc. eventually)
- according to cookbook of ssl: 1) large batch size (above 512) may be instable (probably sharp minimizers?)
2) For large networks, layer-wise dropout (stochastic depth) is helpful especially like 0.5
- For ticker embedding, use the same logic as CLIP if that’s doable. I.e. You randomly sample negative examples,
 and then do some contrastive learning on the tickers. 
- try https://www.evanmiller.org/attention-is-off-by-one.html by adding 1 to the denominator of attention for implicit regularizing
- Go through all the data loading/reading and make it as efficient as possible. and loss calculation. try to parallelize it all.
- Would be cool to see if you can just learn a single D-len vector using sin(L(x)) * sqrt(|L(x)|) + b for time2vec! (SPDER)
- Maybe can hook it up to a vqgan that learns tokens for each category, maybe more expensive tho idk
- idk how to incorporate market data during runtime, maybe if you get a better dataset with other market participants, you can try
 to mask that out and have it trade, then give it that info and use that somehow (maybe RLHF idk). Just think about it, it should
  be fine If we’re not doing high frequency trading.
- for runtime, can add code to finetune on that day's market data to make it more specialized to perform based on recent activity
- at the end, to see how much the network really uses each feature, try to get dLoss/dFeature for each feature (but normalized)
obviously.
- add sentiment analysis feature. Look at the headlines of forbes, fortune, business insider etc for each day (in the past too), and
generate a sentiment vector. Financial roberta?
- You could also pretrain on Math stats proofs to give it priors in that later (i.e. maybe being good at math makes u a better trader?)

Features to include (from GPT):
1. **Price Data**: This includes the high, low, opening, and closing prices.
2. **Volume Data**: This refers to the number of shares or contracts traded in a security or market during a given period.
3. **Volatility**: This can be calculated in a variety of ways, such as using the standard deviation of returns.
4. **Dividend Yield**: This is the annual dividend payment divided by the market price of the stock.
5. **Financial Ratios**: Examples include price-to-earnings (P/E), price-to-book (P/B), and price-to-sales (P/S) ratios, among others. 
6. **Return Data**: This can be calculated in a variety of ways, but commonly used metrics are daily, monthly, or annual returns.
7. **Earnings Data**: This includes earnings per share (EPS), quarterly earnings growth, and other similar metrics.
8. **Market Indicators**: These could be overall market index levels, sector ETF prices, or futures prices, for instance.
also for historical data feature, include the performance of the sector of that stock (and also competitors if possible, alphavantage allows for searching over companies in that industry, so maybe just pick top 5 market cap also, and feed in their last 365 days performance)
9. **Interest Rates**: The level of various interest rates (like the 10-year Treasury rate) can impact the valuation of stocks.
10. **Macroeconomic Data**: Information like GDP growth, unemployment rate, and inflation can impact stock prices.
Maybe for variables like unemployment and gdp growth which are highly correlated and dont vary day to day, just concat them along D-axis. i.e. 0-255 is the value for unemployment, 256-511 is GDP
11. is there a way to get a sentiment analysis metric?
- Using additional technical indicators like Moving Averages, RSI, MACD (probably need alphavantage) if they are not redundant

possible sources:
Alpha Vantage: Alpha Vantage offers free APIs for historical and real-time data on stocks, forex, and cryptocurrencies, as well as technical indicators, sector performances, and other financial metrics. It provides both free and premium tiers. It covers most of your listed requirements like price data, volume data, volatility (through technical indicators), return data, financial ratios, earnings data, and some market indicators. It also provides some basic macroeconomic data.

Quandl: Quandl offers a vast collection of financial, economic, and alternative data that spans over 400,000 datasets from over 400 sources. It also includes a range of premium datasets, some of which might require additional subscription fees. It provides good coverage for price data, return data, volume data, and a variety of financial ratios. Quandl also offers macroeconomic data, including interest rates and GDP.

Intrinio: Intrinio provides access to many different types of securities data, including financial statement data, news, and other alternative datasets. It covers some of your requirements like financial ratios, earnings data, and a wide array of macroeconomic data.

IEX Cloud: IEX Cloud provides real-time & historical stock and market data, as well as a solid set of financial tools. They also offer a free tier, which is a plus.

Finbox: Finbox is another good API service for fetching various types of financial data, including financial ratios, earnings data, and market data. It's a bit more focused on fundamental analysis, so it may not cover all your needs, but it's a good source for deep company data.

FRED (Federal Reserve Economic Data): For macroeconomic data, FRED provides a vast range of time-series data which includes interest rates, GDP, unemployment rate, etc.

News API or GDELT: For sentiment analysis, you could consider using a news API to fetch recent news articles about a company and then use a Natural Language Processing (NLP) service like Google's Natural Language API, IBM Watson, or Microsoft Azure Text Analytics to perform sentiment analysis on the news articles.

Twelve Data: Twelve Data offers a wide range of financial data, including stock market data, forex data, cryptocurrency data, and more. It has over 100 technical indicators and chart types.
